#!/usr/bin/env python3
import re
import os
import csv
import pathlib
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import urllib.parse
import time
import hashlib

# Z√ÅKLADN√ç NASTAVEN√ç
SUBSTACK_DIR = "/Users/patrickzandl/Documents/GitHub/brandys-pirati/substack"
POSTS_DIR = "/Users/patrickzandl/Documents/GitHub/brandys-pirati/_posts/mistostarosti"
ASSETS_DIR = "/Users/patrickzandl/Documents/GitHub/brandys-pirati/assets/mistostarosti"
POSTS_CSV = os.path.join(SUBSTACK_DIR, "posts.csv")
POSTS_HTML_DIR = os.path.join(SUBSTACK_DIR, "posts")

HEADERS = {
    "User-Agent": "Mozilla/5.0 (compatible; Jekyll-converter/1.0)"
}

def remove_diacritics(text):
    """Odstran√≠ diakritiku z textu."""
    diacritic_map = {
        '√°': 'a', '√†': 'a', '√§': 'a', '√¢': 'a', 'ƒÅ': 'a', 'ƒÉ': 'a', 'ƒÖ': 'a',
        '√©': 'e', '√®': 'e', '√´': 'e', '√™': 'e', 'ƒì': 'e', 'ƒõ': 'e', 'ƒô': 'e',
        '√≠': 'i', '√¨': 'i', '√Ø': 'i', '√Æ': 'i', 'ƒ´': 'i',
        '√≥': 'o', '√≤': 'o', '√∂': 'o', '√¥': 'o', '≈ç': 'o', '√∏': 'o',
        '√∫': 'u', '√π': 'u', '√º': 'u', '√ª': 'u', '≈´': 'u', '≈Ø': 'u',
        '√Ω': 'y', '√ø': 'y',
        'ƒç': 'c', 'ƒá': 'c', '√ß': 'c',
        'ƒè': 'd',
        '≈à': 'n', '√±': 'n',
        '≈ô': 'r',
        '≈°': 's', '≈õ': 's',
        '≈•': 't',
        '≈æ': 'z', '≈∫': 'z', '≈º': 'z',
        'ƒæ': 'l', '≈Ç': 'l',
        # velk√° p√≠smena
        '√Å': 'A', '√Ä': 'A', '√Ñ': 'A', '√Ç': 'A', 'ƒÄ': 'A', 'ƒÇ': 'A', 'ƒÑ': 'A',
        '√â': 'E', '√à': 'E', '√ã': 'E', '√ä': 'E', 'ƒí': 'E', 'ƒö': 'E', 'ƒò': 'E',
        '√ç': 'I', '√å': 'I', '√è': 'I', '√é': 'I', 'ƒ™': 'I',
        '√ì': 'O', '√í': 'O', '√ñ': 'O', '√î': 'O', '≈å': 'O', '√ò': 'O',
        '√ö': 'U', '√ô': 'U', '√ú': 'U', '√õ': 'U', '≈™': 'U', '≈Æ': 'U',
        '√ù': 'Y', '≈∏': 'Y',
        'ƒå': 'C', 'ƒÜ': 'C', '√á': 'C',
        'ƒé': 'D',
        '≈á': 'N', '√ë': 'N',
        '≈ò': 'R',
        '≈†': 'S', '≈ö': 'S',
        '≈§': 'T',
        '≈Ω': 'Z', '≈π': 'Z', '≈ª': 'Z',
        'ƒΩ': 'L', '≈Å': 'L'
    }
    
    result = ''
    for char in text:
        result += diacritic_map.get(char, char)
    return result

def create_filename_from_title(title, date_str):
    """Vytvo≈ô√≠ Jekyll filename ve form√°tu YYYY-MM-DD-nazev.md."""
    # parsuj datum z ISO form√°tu
    try:
        date_obj = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
        jekyll_date = date_obj.strftime("%Y-%m-%d")
    except:
        jekyll_date = datetime.now().strftime("%Y-%m-%d")
    
    # vytvo≈ô slug z titulku
    if title:
        title_no_diacritics = remove_diacritics(title)
        slug = re.sub(r'[^\w\s-]', '', title_no_diacritics.lower())
        slug = re.sub(r'[-\s]+', '-', slug).strip('-')
        slug = slug[:50]  # omez d√©lku
    else:
        slug = "mistostarosti"
    
    return f"{jekyll_date}-{slug}.md"

def download_image(img_url, filename):
    """St√°hne obr√°zek a ulo≈æ√≠ do assets/mistostarosti."""
    try:
        response = requests.get(img_url, headers=HEADERS, timeout=30)
        response.raise_for_status()
        
        # vytvo≈ô adres√°≈ô pokud neexistuje
        os.makedirs(ASSETS_DIR, exist_ok=True)
        
        # ulo≈æ soubor
        filepath = os.path.join(ASSETS_DIR, filename)
        with open(filepath, 'wb') as f:
            f.write(response.content)
        
        return f"/assets/mistostarosti/{filename}"
    except Exception as e:
        print(f"    Chyba p≈ôi stahov√°n√≠ obr√°zku {img_url}: {e}")
        return img_url  # vra≈• p≈Øvodn√≠ URL pokud se nepoda≈ôilo st√°hnout

def process_images_in_html(soup, post_title):
    """Zpracuje obr√°zky v HTML a st√°hne je do assets."""
    images = soup.find_all('img')
    
    for img in images:
        src = img.get('src')
        if src and ('substack-post-media.s3.amazonaws.com' in src or 'substackcdn.com' in src):
            # vytvo≈ô n√°zev souboru bez diakritiky
            title_slug = remove_diacritics(post_title.lower())
            title_slug = re.sub(r'[^\w-]', '', title_slug)[:30]
            
            # vytvo≈ô hash z URL pro jedineƒçnost
            url_hash = hashlib.md5(src.encode()).hexdigest()[:8]
            
            # zjisti p≈ô√≠ponu
            parsed_url = urllib.parse.urlparse(src)
            path = parsed_url.path
            ext = '.jpg'  # defaultn√≠
            if '.' in path:
                ext = os.path.splitext(path)[1]
                if not ext:
                    ext = '.jpg'
            
            filename = f"{title_slug}-{url_hash}{ext}"
            
            # st√°hni obr√°zek
            local_path = download_image(src, filename)
            
            # aktualizuj src v HTML
            img['src'] = local_path
            
            # odstra≈à v≈°echny srcset atributy (obsahuj√≠ extern√≠ URL)
            if img.get('srcset'):
                del img['srcset']
            
            print(f"    Obr√°zek sta≈æen: {filename}")

def convert_table_to_markdown(table):
    """P≈ôevede HTML tabulku na Markdown."""
    rows = []
    
    # Najdi v≈°echny ≈ô√°dky
    table_rows = table.find_all("tr")
    if not table_rows:
        return ""
    
    # Zpracuj ≈ô√°dky
    for i, row in enumerate(table_rows):
        cells = row.find_all(["td", "th"])
        if not cells:
            continue
            
        # Z√≠skej text z bunƒõk
        cell_texts = []
        for cell in cells:
            # Rekurzivnƒõ zpracuj obsah bunƒõk (m≈Ø≈æe obsahovat odkazy, form√°tov√°n√≠)
            cell_text = ''.join(convert_element_to_markdown(child) for child in cell.children)
            if not cell_text.strip():
                cell_text = cell.get_text(" ", strip=True)
            # Escape pipes pro Markdown a vyƒçisti
            cell_text = cell_text.replace("|", "\\|").strip()
            cell_texts.append(cell_text)
        
        # Vytvo≈ô ≈ô√°dek tabulky
        if cell_texts:
            row_md = "| " + " | ".join(cell_texts) + " |"
            rows.append(row_md)
            
            # Po prvn√≠m ≈ô√°dku (hlaviƒçka) p≈ôidej oddƒõlovaƒç
            if i == 0:
                separator = "| " + " | ".join(["---"] * len(cell_texts)) + " |"
                rows.append(separator)
    
    return "\n" + "\n".join(rows) + "\n" if rows else ""

def convert_element_to_markdown(element):
    """Rekurzivnƒõ konvertuje HTML element na markdown."""
    if element.name is None:  # text node
        return str(element)
    
    # inline elementy - zachovej form√°tov√°n√≠ s mezerami
    if element.name == 'strong' or element.name == 'b':
        inner_text = ''.join(convert_element_to_markdown(child) for child in element.children)
        return f" **{inner_text.strip()}** "
    
    elif element.name == 'em' or element.name == 'i':
        inner_text = ''.join(convert_element_to_markdown(child) for child in element.children)
        return f" *{inner_text.strip()}* "
    
    elif element.name == 'a':
        inner_text = ''.join(convert_element_to_markdown(child) for child in element.children)
        href = element.get('href', '')
        if href and inner_text:
            return f" [{inner_text.strip()}]({href}) "
        else:
            return inner_text
    
    elif element.name == 'code':
        inner_text = ''.join(convert_element_to_markdown(child) for child in element.children)
        return f" `{inner_text.strip()}` "
    
    # block elementy
    elif element.name == 'p':
        inner_text = ''.join(convert_element_to_markdown(child) for child in element.children)
        return inner_text.strip()
    
    elif element.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
        level = int(element.name[1])
        inner_text = ''.join(convert_element_to_markdown(child) for child in element.children)
        return '#' * level + ' ' + inner_text.strip()
    
    elif element.name == 'blockquote':
        inner_text = ''.join(convert_element_to_markdown(child) for child in element.children)
        lines = inner_text.strip().split('\n')
        quoted_lines = ['> ' + line for line in lines if line.strip()]
        return '\n'.join(quoted_lines)
    
    elif element.name == 'ul':
        items = []
        for li in element.find_all('li', recursive=False):
            item_text = ''.join(convert_element_to_markdown(child) for child in li.children)
            if item_text.strip():
                items.append(f"- {item_text.strip()}")
        return '\n'.join(items)
    
    elif element.name == 'ol':
        items = []
        for i, li in enumerate(element.find_all('li', recursive=False), 1):
            item_text = ''.join(convert_element_to_markdown(child) for child in li.children)
            if item_text.strip():
                items.append(f"{i}. {item_text.strip()}")
        return '\n'.join(items)
    
    elif element.name == 'br':
        return '\n'
    
    # zpracov√°n√≠ tabulek
    elif element.name == 'table':
        return convert_table_to_markdown(element)
    
    # ignoruj inline styling elementy, ale zachovej obsah
    elif element.name in ['span', 'div']:
        return ''.join(convert_element_to_markdown(child) for child in element.children)
    
    # pro nezn√°m√© elementy vra≈• pouze text obsah
    else:
        return ''.join(convert_element_to_markdown(child) for child in element.children)

def convert_html_to_markdown(html_content):
    """Konvertuje Substack HTML na Jekyll markdown."""
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # odstra≈à nepot≈ôebn√© elementy
    try:
        elements_to_remove = []
        for element in soup.find_all(['div', 'span']):
            if element and hasattr(element, 'get') and element.get('class'):
                classes = element.get('class', [])
                if any(cls in str(classes) for cls in ['poll-embed', 'pencraft', 'image-link-expand', 'button-wrapper']):
                    elements_to_remove.append(element)
        
        for element in elements_to_remove:
            element.decompose()
    except Exception as e:
        print(f"    Chyba p≈ôi ƒçi≈°tƒõn√≠ HTML: {e}")
        pass
    
    # konvertuj strukturovan√© elementy
    markdown_parts = []
    content_found = False
    
    for element in soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'blockquote', 'ul', 'ol', 'figure', 'table']):
        try:
            if element.name == 'figure':
                # speci√°ln√≠ zpracov√°n√≠ obr√°zk≈Ø
                img = element.find('img')
                if img and img.get('src'):
                    alt_text = img.get('alt', '') or ''
                    src = img.get('src', '')
                    
                    # najdi caption
                    caption = ''
                    figcaption = element.find('figcaption')
                    if figcaption:
                        caption = figcaption.get_text().strip()
                    
                    # vytvo≈ô markdown pro obr√°zek
                    if caption:
                        markdown_parts.append(f"![{alt_text}]({src})")
                        markdown_parts.append(f"*{caption}*")
                    else:
                        markdown_parts.append(f"![{alt_text}]({src})")
                    content_found = True
            
            elif element.name == 'table':
                # speci√°ln√≠ zpracov√°n√≠ tabulek
                table_markdown = convert_table_to_markdown(element)
                if table_markdown.strip():
                    markdown_parts.append(table_markdown)
                    content_found = True
            
            else:
                # pou≈æij rekurzivn√≠ konverzi
                markdown_text = convert_element_to_markdown(element)
                if markdown_text.strip():
                    # filtruj spam obsah
                    if not any(skip in markdown_text.lower() for skip in ['leave a comment', 'subscribe']):
                        markdown_parts.append(markdown_text)
                        content_found = True
        
        except Exception as e:
            print(f"    Chyba p≈ôi zpracov√°n√≠ elementu {element.name}: {e}")
            continue
    
    # pokud nebyly nalezeny strukturovan√© elementy, pou≈æij prost√Ω text
    if not content_found or not markdown_parts:
        text = soup.get_text().strip()
        if text:
            text = re.sub(r'\s+', ' ', text)
            markdown_parts.append(text)
    
    # zkombinuj s odstavci
    if markdown_parts:
        markdown_content = '\n\n'.join(markdown_parts)
    else:
        markdown_content = soup.get_text().strip()
    
    # vyƒçisti nadbyteƒçn√© pr√°zdn√© ≈ô√°dky a mezery
    markdown_content = re.sub(r'\n{3,}', '\n\n', markdown_content)
    # vyƒçisti duplicitn√≠ mezery ale zachovej jednu mezeru kolem form√°tov√°n√≠
    markdown_content = re.sub(r'  +', ' ', markdown_content)
    # vyƒçisti mezery na zaƒç√°tku a konci ≈ô√°dk≈Ø
    lines = markdown_content.split('\n')
    cleaned_lines = [line.strip() for line in lines]
    markdown_content = '\n'.join(cleaned_lines)
    
    return markdown_content.strip()

def load_posts_metadata():
    """Naƒçte metadata p≈ô√≠spƒõvk≈Ø z CSV."""
    posts = {}
    
    try:
        with open(POSTS_CSV, 'r', encoding='utf-8') as csvfile:
            reader = csv.DictReader(csvfile)
            for row in reader:
                post_id = row['post_id']
                if post_id and row['is_published'] == 'true':
                    posts[post_id] = row
        
        print(f"[*] Naƒçteno {len(posts)} publikovan√Ωch p≈ô√≠spƒõvk≈Ø z CSV")
        return posts
    
    except FileNotFoundError:
        print(f"[!] Soubor {POSTS_CSV} nebyl nalezen!")
        return {}

def process_post(post_id, metadata):
    """Zpracuje jeden p≈ô√≠spƒõvek."""
    # najdi odpov√≠daj√≠c√≠ HTML soubor
    html_files = [f for f in os.listdir(POSTS_HTML_DIR) 
                  if f.startswith(post_id.split('.')[0]) and f.endswith('.html')]
    
    if not html_files:
        print(f"    HTML soubor pro {post_id} nenalezen")
        return False
    
    html_file = html_files[0]
    html_path = os.path.join(POSTS_HTML_DIR, html_file)
    
    try:
        # naƒçti HTML
        with open(html_path, 'r', encoding='utf-8') as f:
            html_content = f.read()
        
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # zpracuj obr√°zky (st√°hni je)
        try:
            process_images_in_html(soup, metadata['title'])
        except Exception as img_error:
            print(f"    Varov√°n√≠: Chyba p≈ôi zpracov√°n√≠ obr√°zk≈Ø: {img_error}")
        
        # konvertuj na markdown
        try:
            markdown_content = convert_html_to_markdown(str(soup))
        except Exception as md_error:
            print(f"    Varov√°n√≠: Chyba p≈ôi konverzi markdown, pou≈æ√≠v√°m prost√Ω text: {md_error}")
            # fallback na prost√Ω text
            markdown_content = soup.get_text().strip()
        
        # vytvo≈ô Jekyll frontmatter
        title = metadata.get('title', '') or 'M√≠stostarosti'
        subtitle = metadata.get('subtitle', '') or ''
        date_str = metadata.get('post_date', '')
        
        # parsuj datum
        try:
            if date_str:
                date_obj = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
                jekyll_date = date_obj.strftime("%Y-%m-%d")
                jekyll_datetime = date_obj.strftime("%Y-%m-%d %H:%M:%S %z")
            else:
                jekyll_date = datetime.now().strftime("%Y-%m-%d")
                jekyll_datetime = datetime.now().strftime("%Y-%m-%d %H:%M:%S %z")
        except Exception as date_error:
            print(f"    Varov√°n√≠: Chyba p≈ôi parsov√°n√≠ data: {date_error}")
            jekyll_date = datetime.now().strftime("%Y-%m-%d")
            jekyll_datetime = datetime.now().strftime("%Y-%m-%d %H:%M:%S %z")
        
        # vytvo≈ô n√°zev souboru
        filename = create_filename_from_title(title, date_str or jekyll_date)
        
        # vytvo≈ô Jekyll frontmatter
        frontmatter = [
            "---",
            "layout: post",
            f'title: "{title.replace('"', '\\"')}"',
            f"date: {jekyll_datetime}",
            f'author: "Patrick Zandl"',
            f'category: "mistostarosti"'
        ]
        
        if subtitle:
            frontmatter.append(f'subtitle: "{subtitle.replace('"', '\\"')}"')
        
        # p≈ôidej odkaz na prvn√≠ obr√°zek jako n√°hled pro tituln√≠ str√°nku
        # najdi prvn√≠ obr√°zek v HTML a zjisti jeho lok√°ln√≠ cestu
        first_image_path = None
        try:
            first_img = soup.find('img')
            if first_img and first_img.get('src'):
                src = first_img.get('src')
                if src.startswith('/assets/mistostarosti/'):
                    first_image_path = src
        except:
            pass
        
        if first_image_path:
            frontmatter.append(f'image: {first_image_path}')
        
        frontmatter.extend([
            f'source: "substack"',
            f'source_id: "{post_id}"',
            "---",
            ""
        ])
        
        # ulo≈æ markdown soubor
        os.makedirs(POSTS_DIR, exist_ok=True)
        output_path = os.path.join(POSTS_DIR, filename)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(frontmatter))
            f.write(markdown_content)
        
        print(f"    ‚úì {filename}")
        return True
        
    except Exception as e:
        print(f"    ‚úó Chyba p≈ôi zpracov√°n√≠ {post_id}: {e}")
        import traceback
        print(f"    Debug info: {traceback.format_exc()}")
        return False

def main():
    print("[*] Konvertuji Substack p≈ô√≠spƒõvky na Jekyll markdown...")
    
    # naƒçti metadata
    posts_metadata = load_posts_metadata()
    
    if not posts_metadata:
        print("[!] ≈Ω√°dn√° metadata k zpracov√°n√≠!")
        return
    
    # zpracuj p≈ô√≠spƒõvky
    processed = 0
    failed = 0
    
    for post_id, metadata in posts_metadata.items():
        print(f"[{processed + failed + 1}/{len(posts_metadata)}] {metadata['title'][:50]}...")
        
        if process_post(post_id, metadata):
            processed += 1
        else:
            failed += 1
        
        # kr√°tk√° pauza mezi po≈æadavky
        time.sleep(0.5)
    
    print(f"\n[*] Dokonƒçeno!")
    print(f"    ‚úì √öspƒõ≈°nƒõ zpracov√°no: {processed} p≈ô√≠spƒõvk≈Ø")
    print(f"    ‚úó Chyby: {failed} p≈ô√≠spƒõvk≈Ø")
    print(f"    üìÅ Soubory ulo≈æeny do: {POSTS_DIR}")
    print(f"    üñºÔ∏è Obr√°zky ulo≈æeny do: {ASSETS_DIR}")

if __name__ == "__main__":
    main()